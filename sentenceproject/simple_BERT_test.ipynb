{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT for next sentence prediction\n",
    "\n",
    "How much difference does OCR make? First pass at the question.\n",
    "\n",
    "The overall strategy here is going to be to construct pairs of actually-sequential sentences from books of biography and fiction. We'll construct pairs using both dirty and clean texts, making sure the sentences match as closely as possible. For each pair we'll calculate the probability that they're really sequential. Of course they all *are* really sequential, so this probability should be high. But some recent papers have argued that there's a systematic difference here, where transitions seem more probable / predictable in fiction.\n",
    "\n",
    "We're not really going to answer that question. But we will ask whether the differences between books, and especially the difference between biography and fiction, are significantly disrupted by text quality.\n",
    "\n",
    "**First, we load BERT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then write a function to do next sentence prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('built tokenizer')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print('built model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_output(firstsentence, secondsentence, tokenizer, model):\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', padding = False)\n",
    "    result = model(**encoding)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_logits(firstsentence, secondsentence, tokenizer, model):\n",
    "\n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', padding = False)\n",
    "    result_object = model(**encoding)\n",
    "    \n",
    "    logits = result_object['logits'].tolist()[0]\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes to this function are necessitated because [HuggingFace has implemented a new way to wrap \"model outputs\"](https://huggingface.co/transformers/main_classes/output.html) since I originally wrote this.\n",
    "\n",
    "You used to get two numeric results. Now you get a NextSentencePredictorOutput, which in turn wraps the results as PyTorch Tensors. Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=None, logits=tensor([[ 6.2713, -6.1164]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_raw_output(firstsentence, secondsentence, tokenizer, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2713, -6.1164]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading [the HuggingFace documentation](https://huggingface.co/transformers/main_classes/output.html) and [Googling Stack Overflow](https://stackoverflow.com/questions/53903373/convert-pytorch-tensor-to-python-list) I was able to write the new get_logits function, which unpacks those objects to get numbers we can deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.2712578773498535, -6.116359710693359]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(firstsentence, secondsentence, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between logits and probability makes my head hurt to explain, so I'm just going to [point at Wikipedia.](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "But for a quick and dirty approach I wrote this function which *loosely* translates BERT's logits output into a probability for the sequence. Also checked [this blog post](https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f) to confirm that the probability of \"yes, this is the next sentence\" is associated with the first logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_probability(firstsent, secondsent):\n",
    "    '''\n",
    "    \n",
    "    :param logits: a tensor produced by BERT\n",
    "    :return: probability of the first category after softmax\n",
    "    '''\n",
    "    global tokenizer, model\n",
    "    \n",
    "    logits = get_logits(firstsent, secondsent, tokenizer, model)\n",
    "    \n",
    "    poslogit = logits[0]\n",
    "    neglogit = logits[1]\n",
    "\n",
    "    pospart = math.pow(2.72, poslogit)\n",
    "    negpart = math.pow(2.72, neglogit)\n",
    "\n",
    "    posprob = pospart / (pospart + negpart)\n",
    "\n",
    "    return round(posprob, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now we can see that BERT considers that a pretty probable sequence. Let's try a less probable sequence.\n",
    "\n",
    "We'll use the same first sentence about walking to the store, and for our second sentence\n",
    "\n",
    "    Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\n",
    "    \n",
    "Which is from Wikipedia on \"psychedelic drug.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much less probable sequence! Let's try a slightly weaker non-sequitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=None, logits=tensor([[ 6.2713, -6.1164]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055497"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Everything is closed due to the pandemic.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that probability is slightly higher. Still unlikely. But not *totally* improbable. It might be higher if BERT had been trained in 2021.\n",
    "\n",
    "## Sentence data\n",
    "\n",
    "Now let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the data has four fields: a Gutenberg index, a row counter that is sequential within a single volume, the dirty_sentence and the clean_sentence.\n",
    "\n",
    "This doesn't read easily into pandas (too many weird characters), so I've written some Python to parse it line by line.\n",
    "\n",
    "This code creates three aligned lists: ```dirtytuples``` (pairs of dirty-ocr sentences), ```cleantuples``` (pairs of clean sentences), and the ```gbindices``` associated with the aligned pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "47811\n"
     ]
    }
   ],
   "source": [
    "oldlen = 0\n",
    "oldcleansent = '<>'\n",
    "olddirtysent = '<>'\n",
    "\n",
    "dirtytuples = []\n",
    "cleantuples = []\n",
    "gbindices = []\n",
    "\n",
    "with open('/Users/tunder/Box Sync/NEHproject/sentencedata/samplesentences.tsv', mode = 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        \n",
    "        if len(fields) != oldlen:\n",
    "            oldlen = len(fields)\n",
    "            print(oldlen)\n",
    "            \n",
    "        if fields[0] == 'gbindex':\n",
    "            continue\n",
    "        elif fields[2].startswith('<match'):\n",
    "            oldcleansent = '<>'\n",
    "            olddirtysent = '<>'\n",
    "        elif oldcleansent == '<>':\n",
    "            olddirtysent = fields[2]\n",
    "            oldcleansent = fields[3]\n",
    "        else:\n",
    "            dirtypair = (olddirtysent, fields[2])\n",
    "            cleanpair = (oldcleansent, fields[3])\n",
    "            gbindex = fields[0]\n",
    "            dirtytuples.append(dirtypair)\n",
    "            cleantuples.append(cleanpair)\n",
    "            gbindices.append(gbindex)\n",
    "            \n",
    "            olddirtysent = fields[2]\n",
    "            oldcleansent = fields[3]\n",
    "\n",
    "print(len(gbindices))          \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rearrange the pairs of sentences into dictionaries keyed by Gutenberg index; the main reason for this is that we're only going to calculate probability for a subset, and we want to distribute that subset randomly but as evenly as possible across volumes.\n",
    "\n",
    "Then we do 4000 iterations, each time selecting a \"key\" (gbindex / volume), and (if there are still pairs of sentences for that volume) a pair of sentences from it. (Really a *pair* of pairs of sentences, clean and dirty.)\n",
    "\n",
    "We calculate probabilities and store them in two dicts, ```gbclean``` and ```gbdirty.```\n",
    "\n",
    "**If the clean and dirty probabilities for a sentence pair diverge by more than .50, we print out the pair. This is a long list, but a few examples can be worth inspection. You'll notice that relatively minor changes (a page number or a misplaced quotation mark) can drastically reduce -- or sometimes, surprisingly, increase -- the measured probability, flipping it from near zero to near one or vice-versa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punct(atuple):\n",
    "    hyphenbreak = False\n",
    "    quotation = False\n",
    "    hasnumeric = False\n",
    "    \n",
    "    for astring in atuple:\n",
    "        if '\"' in astring or '“' in astring or '”' in astring:\n",
    "            quotation = True\n",
    "        if '1' in astring or '0' in astring or '2' in astring or '3' in astring or '4' in astring or '5' in astring:\n",
    "            hasnumeric = True\n",
    "        if '6' in astring or '7' in astring or '8' in astring or '9' in astring:\n",
    "            hasnumeric = True\n",
    "        if '- ' in astring:\n",
    "            hyphenbreak = True\n",
    "    \n",
    "    return hyphenbreak, quotation, hasnumeric\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>isbio</th>\n",
       "      <th>passagefails</th>\n",
       "      <th>worderrors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inu.30000115426458</td>\n",
       "      <td>Fielding, Henry</td>\n",
       "      <td>1707-1754</td>\n",
       "      <td>The history of the adventures of Joseph Andrew...</td>\n",
       "      <td>1742</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>9609</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>n</td>\n",
       "      <td>two volumes</td>\n",
       "      <td>ted</td>\n",
       "      <td>18cgutenbergtrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>False</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.150914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdp.39015014138518</td>\n",
       "      <td>Hawkins, John</td>\n",
       "      <td>1719-1789</td>\n",
       "      <td>The Life of Samuel Johnson, LLD</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>London: J Buckland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HawkJLSbio</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.059192</td>\n",
       "      <td>0.036669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uc2.ark+=13960=t7pn8z31x</td>\n",
       "      <td>Hawthorne, Nathaniel</td>\n",
       "      <td>1804-1864.</td>\n",
       "      <td>Little masterpieces</td>\n",
       "      <td>1864</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>New York;Doubleday &amp; McClure</td>\n",
       "      <td>Hawthorne, Nathaniel | Little Masterpieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39716</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.015333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uc2.ark+=13960=t78s4tc10</td>\n",
       "      <td>Kavanagh, Julia</td>\n",
       "      <td>1824-1877.</td>\n",
       "      <td>Rachel Gray</td>\n",
       "      <td>1856</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>New York;Appleton;1856.</td>\n",
       "      <td>Kavanagh, Julia | Rachel Gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36160</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.074475</td>\n",
       "      <td>0.026068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coo1.ark+=13960=t2t447q6w</td>\n",
       "      <td>Boldrewood, Rolf</td>\n",
       "      <td>1826-1915.</td>\n",
       "      <td>Nevermore</td>\n",
       "      <td>1892</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>London and New York, Macmillan and co., 1892.</td>\n",
       "      <td>Boldrewood, Rolf | Nevermore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34240</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.098180</td>\n",
       "      <td>0.009523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       docid                author  authordate  \\\n",
       "0         inu.30000115426458       Fielding, Henry   1707-1754   \n",
       "1         mdp.39015014138518         Hawkins, John   1719-1789   \n",
       "2   uc2.ark+=13960=t7pn8z31x  Hawthorne, Nathaniel  1804-1864.   \n",
       "3   uc2.ark+=13960=t78s4tc10       Kavanagh, Julia  1824-1877.   \n",
       "4  coo1.ark+=13960=t2t447q6w      Boldrewood, Rolf  1826-1915.   \n",
       "\n",
       "                                               title  latestcomp  hathidate  \\\n",
       "0  The history of the adventures of Joseph Andrew...        1742     1743.0   \n",
       "1                    The Life of Samuel Johnson, LLD        1787     1787.0   \n",
       "2                                Little masterpieces        1864     1897.0   \n",
       "3                                        Rachel Gray        1856     1856.0   \n",
       "4                                          Nevermore        1892     1892.0   \n",
       "\n",
       "                                         imprint  \\\n",
       "0                                            NaN   \n",
       "1                             London: J Buckland   \n",
       "2                   New York;Doubleday & McClure   \n",
       "3                        New York;Appleton;1856.   \n",
       "4  London and New York, Macmillan and co., 1892.   \n",
       "\n",
       "                                  gutenstring enumcron     gbindex  ...  \\\n",
       "0                                         NaN        2        9609  ...   \n",
       "1                                         NaN      NaN  HawkJLSbio  ...   \n",
       "2  Hawthorne, Nathaniel | Little Masterpieces      NaN       39716  ...   \n",
       "3               Kavanagh, Julia | Rachel Gray      NaN       36160  ...   \n",
       "4                Boldrewood, Rolf | Nevermore      NaN       34240  ...   \n",
       "\n",
       "   audience  authgender  multiplehtids     comments    coder  \\\n",
       "0       NaN           m              n  two volumes      ted   \n",
       "1       NaN           m            NaN          NaN      NaN   \n",
       "2       NaN           m            NaN          NaN  peizhen   \n",
       "3       NaN           f            NaN          NaN  peizhen   \n",
       "4       NaN           m            NaN          NaN      ted   \n",
       "\n",
       "                Folder  Trimmed  isbio passagefails worderrors  \n",
       "0  18cgutenbergtrimmed  Trimmed  False     0.028328   0.150914  \n",
       "1                  NaN      NaN   True     0.059192   0.036669  \n",
       "2                  NaN      NaN  False     0.025850   0.015333  \n",
       "3                  NaN      NaN  False     0.074475   0.026068  \n",
       "4                  NaN      NaN  False     0.098180   0.009523  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('sentencemeta.tsv', sep = '\\t')\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "1 826\n",
      "40010  is dead.\n",
      "6593  is dead.\n",
      "1001 52\n",
      "75 3996\n",
      "clean:  Ttest_indResult(statistic=-2.950817366318367, pvalue=0.003206265691946096)\n",
      "dirty:  Ttest_indResult(statistic=-3.8504681108883654, pvalue=0.00012160614323496583)\n",
      "\n",
      "1\n",
      "1 885\n",
      "40010  is dead.\n",
      "6593  is dead.\n",
      "1001 908\n",
      "RichSPO3fic  is dead.\n",
      "31473  is dead.\n",
      "158 3992\n",
      "clean:  Ttest_indResult(statistic=-1.7620117413791672, pvalue=0.07822052050629599)\n",
      "dirty:  Ttest_indResult(statistic=-2.0632009528340474, pvalue=0.03922272228497866)\n",
      "\n",
      "2\n",
      "RichSPO3fic  is dead.\n",
      "1 0\n",
      "40010  is dead.\n",
      "9609  is dead.\n",
      "6593  is dead.\n",
      "CumbRAE2bio  is dead.\n",
      "31473  is dead.\n",
      "1001 810\n",
      "56605  is dead.\n",
      "DefoDLSfic  is dead.\n",
      "228 3984\n",
      "clean:  Ttest_indResult(statistic=-2.091695868409204, pvalue=0.036592208259770664)\n",
      "dirty:  Ttest_indResult(statistic=-2.157038474729613, pvalue=0.031122066507428478)\n",
      "\n",
      "3\n",
      "1 893\n",
      "CumbRAE2bio  is dead.\n",
      "DefoDLSfic  is dead.\n",
      "RichSPO3fic  is dead.\n",
      "9609  is dead.\n",
      "40010  is dead.\n",
      "6593  is dead.\n",
      "56605  is dead.\n",
      "31473  is dead.\n",
      "1001 845\n",
      "49332  is dead.\n",
      "311 3982\n",
      "clean:  Ttest_indResult(statistic=-2.2972289609475443, pvalue=0.02170912849446504)\n",
      "dirty:  Ttest_indResult(statistic=-3.295602478049022, pvalue=0.0009993724109970553)\n",
      "\n",
      "4\n",
      "1 478\n",
      "RichSPO3fic  is dead.\n",
      "56605  is dead.\n",
      "40010  is dead.\n",
      "9609  is dead.\n",
      "31473  is dead.\n",
      "6593  is dead.\n",
      "32424  is dead.\n",
      "DefoDLSfic  is dead.\n",
      "1001 270\n",
      "395 3984\n",
      "clean:  Ttest_indResult(statistic=-4.891856206904124, pvalue=1.0791209817882291e-06)\n",
      "dirty:  Ttest_indResult(statistic=-5.768695434430003, pvalue=9.242537176296261e-09)\n"
     ]
    }
   ],
   "source": [
    "gbdict = dict()\n",
    "\n",
    "ctr = 0\n",
    "for a, b, c in zip(gbindices, cleantuples, dirtytuples):\n",
    "    if a not in gbdict:\n",
    "        gbdict[a] = []\n",
    "    gbdict[a].append((b, c, ctr))\n",
    "    ctr += 1\n",
    "\n",
    "countersrecorded = set()\n",
    "tetragramata = []\n",
    "\n",
    "allhyphen = []\n",
    "allnumeric = []\n",
    "allquote = []\n",
    "allall = []\n",
    "\n",
    "allficcleanprobs = []\n",
    "allbiocleanprobs = []\n",
    "allficdirtyprobs = []\n",
    "allbiodirtyprobs = []\n",
    "\n",
    "for outeriteration in range(5):\n",
    "    print()\n",
    "    print(outeriteration)\n",
    "    keycount = 0\n",
    "    keylist = []\n",
    "\n",
    "    for key in gbdict.keys():\n",
    "        random.shuffle(gbdict[key])\n",
    "        keycount += 1\n",
    "        keylist.append(key)\n",
    "\n",
    "    keylist = random.sample(keylist, 72)\n",
    "\n",
    "    gbclean = dict()\n",
    "    gbdirty = dict()\n",
    "    \n",
    "    hyphenprobs = []\n",
    "    numericprobs = []\n",
    "    quoteprobs = []\n",
    "    allprobs = []\n",
    "\n",
    "    for i in range(2000):\n",
    "\n",
    "        if i % 1000 == 1:\n",
    "            print(i, len(gbdict[key]))\n",
    "\n",
    "        key = random.choice(keylist)\n",
    "\n",
    "        if len(gbdict[key]) < 1:\n",
    "            keylist.pop(keylist.index(key))\n",
    "            print(key, \" is dead.\")\n",
    "            continue\n",
    "\n",
    "        else:     \n",
    "            cleantuple, dirtytuple, counteridx = gbdict[key].pop(-1)\n",
    "\n",
    "        if key not in gbclean:\n",
    "            gbclean[key] = []\n",
    "            gbdirty[key] = []\n",
    "\n",
    "        dirtyprob = get_probability(dirtytuple[0], dirtytuple[1])\n",
    "        cleanprob = get_probability(cleantuple[0], cleantuple[1])\n",
    "        \n",
    "        hyphenbreak, quotation, hasnumeric = check_punct(dirtytuple)\n",
    "        \n",
    "        allprobs.append(dirtyprob)\n",
    "        if hyphenbreak:\n",
    "            hyphenprobs.append(dirtyprob)\n",
    "        if quotation:\n",
    "            quoteprobs.append(dirtyprob)\n",
    "        if hasnumeric:\n",
    "            numericprobs.append(dirtyprob)\n",
    "        \n",
    "        allprobs.append(cleanprob)\n",
    "        hyphenbreak, quotation, hasnumeric = check_punct(cleantuple)\n",
    "        if hyphenbreak:\n",
    "            hyphenprobs.append(cleanprob)\n",
    "        if quotation:\n",
    "            quoteprobs.append(cleanprob)\n",
    "        if hasnumeric:\n",
    "            numericprobs.append(cleanprob)\n",
    "\n",
    "        gbdirty[key].append(dirtyprob)\n",
    "        gbclean[key].append(cleanprob)\n",
    "\n",
    "        if abs(dirtyprob-cleanprob) > .25 and counteridx not in countersrecorded:\n",
    "            tetragram = (cleantuple, dirtytuple, round(cleanprob, 4), round(dirtyprob, 4))\n",
    "            countersrecorded.add(counteridx)\n",
    "            tetragramata.append(tetragram)\n",
    "        \n",
    "    print(len(tetragramata), len(allprobs))\n",
    "\n",
    "    keylist = list(gbclean.keys())\n",
    "    \n",
    "    ficcleanprobs = []\n",
    "    biocleanprobs = []\n",
    "    ficdirtyprobs = []\n",
    "    biodirtyprobs = []\n",
    "\n",
    "    for key in keylist:\n",
    "        genre = meta.loc[meta.gbindex == key, 'genre'].values[0]\n",
    "        if 'fic' in genre:\n",
    "            ficcleanprobs.extend(gbclean[key])\n",
    "            ficdirtyprobs.extend(gbdirty[key])\n",
    "        elif 'bio' in genre:\n",
    "            biocleanprobs.extend(gbclean[key])\n",
    "            biodirtyprobs.extend(gbdirty[key])\n",
    "        else:\n",
    "            print(genre)\n",
    "\n",
    "    print('clean: ', ttest_ind(ficcleanprobs, biocleanprobs))\n",
    "    print('dirty: ', ttest_ind(ficdirtyprobs, biodirtyprobs))\n",
    "    \n",
    "    allficcleanprobs.append(ficcleanprobs)\n",
    "    allbiocleanprobs.append(biocleanprobs)\n",
    "    allficdirtyprobs.append(ficdirtyprobs)\n",
    "    allbiodirtyprobs.append(biodirtyprobs)\n",
    "    \n",
    "    allhyphen.append(hyphenprobs)\n",
    "    allquote.append(quoteprobs)\n",
    "    allnumeric.append(numericprobs)\n",
    "    allall.append(allprobs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "ficbio = []\n",
    "dirtyclean = []\n",
    "subset = []\n",
    "\n",
    "anan = float('nan')\n",
    "\n",
    "for i in range(5):\n",
    "    means.append(np.mean(allhyphen[i]))\n",
    "    ficbio.append(anan)\n",
    "    dirtyclean.append(anan)\n",
    "    subset.append('hyphen')\n",
    "    \n",
    "    means.append(np.mean(allquote[i]))\n",
    "    ficbio.append(anan)\n",
    "    dirtyclean.append(anan)\n",
    "    subset.append('quote')\n",
    "    \n",
    "    means.append(np.mean(allnumeric[i]))\n",
    "    ficbio.append(anan)\n",
    "    dirtyclean.append(anan)\n",
    "    subset.append('numeric')\n",
    "    \n",
    "    means.append(np.mean(allall[i]))\n",
    "    ficbio.append(anan)\n",
    "    dirtyclean.append(anan)\n",
    "    subset.append('all')\n",
    "    \n",
    "    \n",
    "    means.append(np.mean(allficcleanprobs[i]))\n",
    "    ficbio.append('fic')\n",
    "    dirtyclean.append('clean')\n",
    "    subset.append('cleanfic')\n",
    "    \n",
    "    means.append(np.mean(allficdirtyprobs[i]))\n",
    "    ficbio.append('fic')\n",
    "    dirtyclean.append('dirty')\n",
    "    subset.append('dirtyfic')\n",
    "    \n",
    "    means.append(np.mean(allbiocleanprobs[i]))\n",
    "    ficbio.append('bio')\n",
    "    dirtyclean.append('clean')\n",
    "    subset.append('cleanbio')\n",
    "    \n",
    "    means.append(np.mean(allbiodirtyprobs[i]))\n",
    "    ficbio.append('bio')\n",
    "    dirtyclean.append('dirty')\n",
    "    subset.append('dirtybio')\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({'mean': means, 'subset': subset, 'ficbio': ficbio, 'dirtyclean': dirtyclean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='subset', ylabel='mean'>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (9, 6))\n",
    "sns.swarmplot(data = df, x = 'subset', y = 'mean', size = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93801425390625"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(numericprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278837541778076"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(allprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aseries in [allficcleanp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess results\n",
    "\n",
    "Are the books roughly in the same order no matter whether we use dirty or clean texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson:  (0.9527454884159031, 9.395404391083857e-11)\n",
      "Spearman:  SpearmanrResult(correlation=0.9744360902255638, pvalue=4.0365222137155066e-13)\n"
     ]
    }
   ],
   "source": [
    "keylist = list(gbclean.keys())\n",
    "\n",
    "cleanprobs = []\n",
    "dirtyprobs = []\n",
    "\n",
    "for key in keylist:\n",
    "    cleanprobs.append(np.mean(gbclean[key]))\n",
    "    dirtyprobs.append(np.mean(gbdirty[key]))\n",
    "\n",
    "print('Pearson: ', pearsonr(cleanprobs, dirtyprobs))\n",
    "print('Spearman: ', spearmanr(cleanprobs, dirtyprobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yes, correlation between the book averages is very high.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if we compare biography to fiction?\n",
    "\n",
    "Read in metadata so we can infer genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>isbio</th>\n",
       "      <th>passagefails</th>\n",
       "      <th>worderrors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inu.30000115426458</td>\n",
       "      <td>Fielding, Henry</td>\n",
       "      <td>1707-1754</td>\n",
       "      <td>The history of the adventures of Joseph Andrew...</td>\n",
       "      <td>1742</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>9609</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>n</td>\n",
       "      <td>two volumes</td>\n",
       "      <td>ted</td>\n",
       "      <td>18cgutenbergtrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>False</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.150914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdp.39015014138518</td>\n",
       "      <td>Hawkins, John</td>\n",
       "      <td>1719-1789</td>\n",
       "      <td>The Life of Samuel Johnson, LLD</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>London: J Buckland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HawkJLSbio</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.059192</td>\n",
       "      <td>0.036669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uc2.ark+=13960=t7pn8z31x</td>\n",
       "      <td>Hawthorne, Nathaniel</td>\n",
       "      <td>1804-1864.</td>\n",
       "      <td>Little masterpieces</td>\n",
       "      <td>1864</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>New York;Doubleday &amp; McClure</td>\n",
       "      <td>Hawthorne, Nathaniel | Little Masterpieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39716</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.015333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uc2.ark+=13960=t78s4tc10</td>\n",
       "      <td>Kavanagh, Julia</td>\n",
       "      <td>1824-1877.</td>\n",
       "      <td>Rachel Gray</td>\n",
       "      <td>1856</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>New York;Appleton;1856.</td>\n",
       "      <td>Kavanagh, Julia | Rachel Gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36160</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.074475</td>\n",
       "      <td>0.026068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coo1.ark+=13960=t2t447q6w</td>\n",
       "      <td>Boldrewood, Rolf</td>\n",
       "      <td>1826-1915.</td>\n",
       "      <td>Nevermore</td>\n",
       "      <td>1892</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>London and New York, Macmillan and co., 1892.</td>\n",
       "      <td>Boldrewood, Rolf | Nevermore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34240</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.098180</td>\n",
       "      <td>0.009523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       docid                author  authordate  \\\n",
       "0         inu.30000115426458       Fielding, Henry   1707-1754   \n",
       "1         mdp.39015014138518         Hawkins, John   1719-1789   \n",
       "2   uc2.ark+=13960=t7pn8z31x  Hawthorne, Nathaniel  1804-1864.   \n",
       "3   uc2.ark+=13960=t78s4tc10       Kavanagh, Julia  1824-1877.   \n",
       "4  coo1.ark+=13960=t2t447q6w      Boldrewood, Rolf  1826-1915.   \n",
       "\n",
       "                                               title  latestcomp  hathidate  \\\n",
       "0  The history of the adventures of Joseph Andrew...        1742     1743.0   \n",
       "1                    The Life of Samuel Johnson, LLD        1787     1787.0   \n",
       "2                                Little masterpieces        1864     1897.0   \n",
       "3                                        Rachel Gray        1856     1856.0   \n",
       "4                                          Nevermore        1892     1892.0   \n",
       "\n",
       "                                         imprint  \\\n",
       "0                                            NaN   \n",
       "1                             London: J Buckland   \n",
       "2                   New York;Doubleday & McClure   \n",
       "3                        New York;Appleton;1856.   \n",
       "4  London and New York, Macmillan and co., 1892.   \n",
       "\n",
       "                                  gutenstring enumcron     gbindex  ...  \\\n",
       "0                                         NaN        2        9609  ...   \n",
       "1                                         NaN      NaN  HawkJLSbio  ...   \n",
       "2  Hawthorne, Nathaniel | Little Masterpieces      NaN       39716  ...   \n",
       "3               Kavanagh, Julia | Rachel Gray      NaN       36160  ...   \n",
       "4                Boldrewood, Rolf | Nevermore      NaN       34240  ...   \n",
       "\n",
       "   audience  authgender  multiplehtids     comments    coder  \\\n",
       "0       NaN           m              n  two volumes      ted   \n",
       "1       NaN           m            NaN          NaN      NaN   \n",
       "2       NaN           m            NaN          NaN  peizhen   \n",
       "3       NaN           f            NaN          NaN  peizhen   \n",
       "4       NaN           m            NaN          NaN      ted   \n",
       "\n",
       "                Folder  Trimmed  isbio passagefails worderrors  \n",
       "0  18cgutenbergtrimmed  Trimmed  False     0.028328   0.150914  \n",
       "1                  NaN      NaN   True     0.059192   0.036669  \n",
       "2                  NaN      NaN  False     0.025850   0.015333  \n",
       "3                  NaN      NaN  False     0.074475   0.026068  \n",
       "4                  NaN      NaN  False     0.098180   0.009523  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('sentencemetav1.tsv', sep = '\\t')\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['docid', 'author', 'authordate', 'title', 'latestcomp', 'hathidate',\n",
       "       'imprint', 'gutenstring', 'enumcron', 'gbindex', 'nonficprob',\n",
       "       'juvenileprob', 'LOCgenres', 'LOCsubjects', 'contents', 'instances',\n",
       "       'genre', 'audience', 'authgender', 'multiplehtids', 'comments', 'coder',\n",
       "       'Folder', 'Trimmed', 'isbio', 'passagefails', 'worderrors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reorganize the probabilities-by-volume into master lists for fiction and biography, and then do t tests on the paired lists for both clean and dirty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean:  Ttest_indResult(statistic=-1.2552356633919197, pvalue=0.209466695373298)\n",
      "dirty:  Ttest_indResult(statistic=-2.5526956935666116, pvalue=0.010726182227341407)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "keylist = list(gbclean.keys())\n",
    "ficcleanprobs = []\n",
    "biocleanprobs = []\n",
    "ficdirtyprobs = []\n",
    "biodirtyprobs = []\n",
    "\n",
    "for key in keylist:\n",
    "    genre = meta.loc[meta.gbindex == key, 'genre'].values[0]\n",
    "    if 'fic' in genre:\n",
    "        ficcleanprobs.extend(gbclean[key])\n",
    "        ficdirtyprobs.extend(gbdirty[key])\n",
    "    elif 'bio' in genre:\n",
    "        biocleanprobs.extend(gbclean[key])\n",
    "        biodirtyprobs.extend(gbdirty[key])\n",
    "    else:\n",
    "        print(genre)\n",
    "        \n",
    "print('clean: ', ttest_ind(ficcleanprobs, biocleanprobs))\n",
    "print('dirty: ', ttest_ind(ficdirtyprobs, biodirtyprobs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow, that could be a distortion.** We get a significant difference between biography and fiction, but only using the dirty data!\n",
    "\n",
    "What are the actual means we're talking about here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937915840296496 0.9471690701195219\n",
      "0.9243410377358491 0.9441725521912351\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ficcleanprobs), np.mean(biocleanprobs))\n",
    "print(np.mean(ficdirtyprobs), np.mean(biodirtyprobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One working hypothesis here is that quotation marks tend to reduce perceived probability of transition. They're more common in fiction to start with, and *misplaced* quotation marks seem to be especially fatal. So there could be something about next-sentence prediction, and about the bio/fic boundary, that makes bad OCR esp fatal here.\n",
    "\n",
    "It's also notable that dirty probabilities are generally (and significantly) lower than clean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
