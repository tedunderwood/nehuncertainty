{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binary-feature",
   "metadata": {},
   "source": [
    "# A template for experiments\n",
    "\n",
    "I'm trying to develop a model we can use for experiments on the NEH data. But this is by no means set in stone yet; it's a first draft we should discuss and adjust.\n",
    "\n",
    "For a first test, let's consider the problem of author gender. We know our model of gender is imperfect, and we don't imagine a predictive model trained on this boundary will tell us very much about gender directly; it's almost certainly, to some degree, a proxy for genre. But it's a tricky boundary to model and thus a good place to start. We're in no danger of getting 100% accuracy!\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "suspected-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-hollywood",
   "metadata": {},
   "source": [
    "### Title metadata\n",
    "\n",
    "Eventually we will have detailed metadata for each \"chunk\" of a title, reporting things like error levels, the ratio between the lengths of clean-text and ocr-text chunks, etc.\n",
    "\n",
    "Right now I haven't created that yet. So our strategy for getting metadata will be to import title-level metadata and then map it out to the chunks we observe, using the heuristic that each chunk is named according to the formula ```gbindex_chunknumber.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extended-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "titlemeta = pd.read_csv('../metadata/cleanrowswithhathimatches.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "physical-longitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>contents</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvd.32044070870779</td>\n",
       "      <td>Smiles, Samuel,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lives of the engineers</td>\n",
       "      <td>1879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;J. Murray;1874-1877.</td>\n",
       "      <td>Smiles, Samuel | Lives of the Engineers</td>\n",
       "      <td>v. 5</td>\n",
       "      <td>27710</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio | short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 people, mixed together (not one per chapter)</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015005892362</td>\n",
       "      <td>Cruttwell, Maud.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Luca Signorelli</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;G. Bell &amp; sons;1899.</td>\n",
       "      <td>Cruttwell, Maud | Luca Signorelli</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27759</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015051108531</td>\n",
       "      <td>Bettany, George Thomas,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life of Charles Darwin</td>\n",
       "      <td>1887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;W. Scott;1887.</td>\n",
       "      <td>Bettany, George Thomas | Life of Charles Darwin</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28380</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loc.ark+=13960=t6b27z54n</td>\n",
       "      <td>Gay, Sydney Howard,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James Madison</td>\n",
       "      <td>1889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boston;New York;Houghton, Mi</td>\n",
       "      <td>Gay, Sydney Howard | James Madison</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28992</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid                   author authordate  \\\n",
       "0  loc.ark+=13960=t5p851b8s          Reid, Stuart J.        NaN   \n",
       "1        hvd.32044070870779          Smiles, Samuel,        NaN   \n",
       "2        mdp.39015005892362         Cruttwell, Maud.        NaN   \n",
       "3        mdp.39015051108531  Bettany, George Thomas,        NaN   \n",
       "4  loc.ark+=13960=t6b27z54n      Gay, Sydney Howard,        NaN   \n",
       "\n",
       "                    title  latestcomp  hathidate  \\\n",
       "0       Lord John Russell        1895        NaN   \n",
       "1  Lives of the engineers        1879        NaN   \n",
       "2         Luca Signorelli        1899        NaN   \n",
       "3  Life of Charles Darwin        1887        NaN   \n",
       "4           James Madison        1889        NaN   \n",
       "\n",
       "                        imprint  \\\n",
       "0  New York;Harper & brothers;1   \n",
       "1   London;J. Murray;1874-1877.   \n",
       "2   London;G. Bell & sons;1899.   \n",
       "3         London;W. Scott;1887.   \n",
       "4  Boston;New York;Houghton, Mi   \n",
       "\n",
       "                                       gutenstring enumcron gbindex  ...  \\\n",
       "0              Reid, Stuart J. | Lord John Russell  <blank>   27553  ...   \n",
       "1          Smiles, Samuel | Lives of the Engineers     v. 5   27710  ...   \n",
       "2                Cruttwell, Maud | Luca Signorelli  <blank>   27759  ...   \n",
       "3  Bettany, George Thomas | Life of Charles Darwin  <blank>   28380  ...   \n",
       "4               Gay, Sydney Howard | James Madison  <blank>   28992  ...   \n",
       "\n",
       "   contents  instances        genre audience authgender  multiplehtids  \\\n",
       "0       NaN        NaN          bio      NaN          u            NaN   \n",
       "1       NaN        NaN  bio | short      NaN          m            NaN   \n",
       "2       NaN        NaN          bio      NaN          f            NaN   \n",
       "3       NaN        NaN          bio      NaN          m            NaN   \n",
       "4       NaN        NaN          bio      NaN          u            NaN   \n",
       "\n",
       "                                         comments   coder           Folder  \\\n",
       "0                                             NaN  morgan  gutenbiotrimmed   \n",
       "1  2 people, mixed together (not one per chapter)  morgan  gutenbiotrimmed   \n",
       "2                                             NaN  morgan  gutenbiotrimmed   \n",
       "3                                             NaN  morgan  gutenbiotrimmed   \n",
       "4                                             NaN  morgan  gutenbiotrimmed   \n",
       "\n",
       "   Trimmed  \n",
       "0  Trimmed  \n",
       "1  Trimmed  \n",
       "2  Trimmed  \n",
       "3  Trimmed  \n",
       "4  Trimmed  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlemeta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "threaded-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 24)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlemeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-vampire",
   "metadata": {},
   "source": [
    "### Works we have actually trimmed and chunked so far\n",
    "\n",
    "Let's limit the titles to those present in the Box folders that store processed texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "judicial-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/cleannarratives/')\n",
    "              if x.endswith('.txt')]\n",
    "dirtyfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/dirtynarratives/')\n",
    "               if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1237 separate chunks.\n"
     ]
    }
   ],
   "source": [
    "assert len(cleanfiles) == len(dirtyfiles)\n",
    "print(\"We have \" + str(len(cleanfiles)) + \" separate chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-scientist",
   "metadata": {},
   "source": [
    "What is actually in these data objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-content",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36965_3.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanfiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-subscriber",
   "metadata": {},
   "source": [
    "How many titles have we trimmed and chunked so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regular-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbindex(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "gbdict = dict()\n",
    "\n",
    "for filename in cleanfiles:\n",
    "    gbindex = get_gbindex(filename)\n",
    "    if gbindex not in gbdict:\n",
    "        gbdict[gbindex] = []\n",
    "    gbdict[gbindex].append(filename)\n",
    "\n",
    "gbset = set(gbdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thousand-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But only 223 separate titles.\n"
     ]
    }
   ],
   "source": [
    "print(\"But only \" + str(len(gbset)) + \" separate titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surprised-halifax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 24)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = titlemeta.loc[titlemeta['gbindex'].isin(gbset), : ]\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-pasta",
   "metadata": {},
   "source": [
    "Lol, contrary to my assertion that ```cleanrowswithhathimatches``` has one row for each gbindex, there appear to still be duplicated gbindexes. I will need to fix that in the original. **(In fact, there's still a lot of work to be done on the original metadata.)** For right now, a kludge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "raised-branch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = ourtitles.drop_duplicates(subset = 'gbindex')\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operational-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>contents</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>uc1.$b288364</td>\n",
       "      <td>Church, Alfred John</td>\n",
       "      <td>1829-1912.</td>\n",
       "      <td>Stories of the Old world</td>\n",
       "      <td>1884</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>Boston;Ginn, Heath &amp; co.;188</td>\n",
       "      <td>Church, Alfred John | Stories of the Old world</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43982</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic | folklore | short</td>\n",
       "      <td>juv</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ted</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>uc2.ark+=13960=t22b8z55p</td>\n",
       "      <td>Carter, Harry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The autobiography of a Cornish smuggler</td>\n",
       "      <td>1809</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>London;Gibbings &amp; Co.;Truro;</td>\n",
       "      <td>Carter, Harry | The Autobiography of a Cornish...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40008</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ted</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>uc2.ark+=13960=t7wm14b10</td>\n",
       "      <td>Reed, Myrtle</td>\n",
       "      <td>1874-1911.</td>\n",
       "      <td>Master of the vineyard</td>\n",
       "      <td>1910</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>New York;G.P. Putnam;1910.</td>\n",
       "      <td>Reed, Myrtle | Master of the Vineyard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27661</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>fic | romance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>uc2.ark+=13960=t5cc0wf99</td>\n",
       "      <td>Tracy, Louis</td>\n",
       "      <td>1863-1928.</td>\n",
       "      <td>Cynthia's chauffeur</td>\n",
       "      <td>1910</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>New York;Grosset &amp; Dunlap;c1</td>\n",
       "      <td>Tracy, Louis | Cynthia's Chauffeur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31472</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>umn.319510019644103</td>\n",
       "      <td>Stratton, Royal B</td>\n",
       "      <td>d. 1875.</td>\n",
       "      <td>Captivity of the Oatman girls:</td>\n",
       "      <td>1858</td>\n",
       "      <td>1858.0</td>\n",
       "      <td>New-York;Pub. for the author</td>\n",
       "      <td>Stratton, Royal B. | Captivity of the Oatman G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55071</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>njp.32101066164219</td>\n",
       "      <td>Grimaldi, Joseph,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Memoirs of Joseph Grimaldi</td>\n",
       "      <td>1846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;R. Bentley;1846.</td>\n",
       "      <td>Grimaldi, Joseph | Memoirs of Joseph Grimaldi</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>46709</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>osu.32435002999571</td>\n",
       "      <td>Kennedy, John Pendleton</td>\n",
       "      <td>1795-1870.</td>\n",
       "      <td>Horse-shoe Robinson</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>London;Bentley;1835.</td>\n",
       "      <td>Kennedy, John Pendleton | Horse-Shoe Robinson</td>\n",
       "      <td>vol.1</td>\n",
       "      <td>33478</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic | historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peizhen</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mdp.39015061869783</td>\n",
       "      <td>Tappan, Eva March</td>\n",
       "      <td>1854-1930.</td>\n",
       "      <td>In the days of Queen Victoria</td>\n",
       "      <td>1903</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>Boston;Lothrop, Lee &amp; Shepar</td>\n",
       "      <td>Tappan, Eva March | In the Days of Queen Victoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35576</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>nyp.33433074932322</td>\n",
       "      <td>Barr, Robert</td>\n",
       "      <td>1850-1912.</td>\n",
       "      <td>The speculations of John Steele</td>\n",
       "      <td>1905</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>New York;F. A. Stokes compan</td>\n",
       "      <td>Barr, Robert | The Speculations of John Steele</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55328</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ted</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>uc2.ark+=13960=t27942z7w</td>\n",
       "      <td>Jenkins, Hester Donaldson,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ibrahim Pasha</td>\n",
       "      <td>1911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Columbia University</td>\n",
       "      <td>Jenkins, Hester Donaldson | Ibrahim Pasha</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>51299</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        docid                      author  authordate  \\\n",
       "213              uc1.$b288364         Church, Alfred John  1829-1912.   \n",
       "43   uc2.ark+=13960=t22b8z55p               Carter, Harry         NaN   \n",
       "454  uc2.ark+=13960=t7wm14b10                Reed, Myrtle  1874-1911.   \n",
       "455  uc2.ark+=13960=t5cc0wf99                Tracy, Louis  1863-1928.   \n",
       "83        umn.319510019644103           Stratton, Royal B    d. 1875.   \n",
       "66         njp.32101066164219           Grimaldi, Joseph,         NaN   \n",
       "129        osu.32435002999571     Kennedy, John Pendleton  1795-1870.   \n",
       "21         mdp.39015061869783           Tappan, Eva March  1854-1930.   \n",
       "369        nyp.33433074932322                Barr, Robert  1850-1912.   \n",
       "77   uc2.ark+=13960=t27942z7w  Jenkins, Hester Donaldson,         NaN   \n",
       "\n",
       "                                       title  latestcomp  hathidate  \\\n",
       "213                 Stories of the Old world        1884     1884.0   \n",
       "43   The autobiography of a Cornish smuggler        1809     1900.0   \n",
       "454                   Master of the vineyard        1910     1910.0   \n",
       "455                      Cynthia's chauffeur        1910     1910.0   \n",
       "83            Captivity of the Oatman girls:        1858     1858.0   \n",
       "66                Memoirs of Joseph Grimaldi        1846        NaN   \n",
       "129                      Horse-shoe Robinson        1835     1835.0   \n",
       "21             In the days of Queen Victoria        1903     1903.0   \n",
       "369          The speculations of John Steele        1905     1905.0   \n",
       "77                             Ibrahim Pasha        1911        NaN   \n",
       "\n",
       "                          imprint  \\\n",
       "213  Boston;Ginn, Heath & co.;188   \n",
       "43   London;Gibbings & Co.;Truro;   \n",
       "454    New York;G.P. Putnam;1910.   \n",
       "455  New York;Grosset & Dunlap;c1   \n",
       "83   New-York;Pub. for the author   \n",
       "66        London;R. Bentley;1846.   \n",
       "129          London;Bentley;1835.   \n",
       "21   Boston;Lothrop, Lee & Shepar   \n",
       "369  New York;F. A. Stokes compan   \n",
       "77   New York;Columbia University   \n",
       "\n",
       "                                           gutenstring enumcron gbindex  ...  \\\n",
       "213     Church, Alfred John | Stories of the Old world      NaN   43982  ...   \n",
       "43   Carter, Harry | The Autobiography of a Cornish...      NaN   40008  ...   \n",
       "454              Reed, Myrtle | Master of the Vineyard      NaN   27661  ...   \n",
       "455                 Tracy, Louis | Cynthia's Chauffeur      NaN   31472  ...   \n",
       "83   Stratton, Royal B. | Captivity of the Oatman G...      NaN   55071  ...   \n",
       "66       Grimaldi, Joseph | Memoirs of Joseph Grimaldi  <blank>   46709  ...   \n",
       "129      Kennedy, John Pendleton | Horse-Shoe Robinson    vol.1   33478  ...   \n",
       "21   Tappan, Eva March | In the Days of Queen Victoria      NaN   35576  ...   \n",
       "369     Barr, Robert | The Speculations of John Steele      NaN   55328  ...   \n",
       "77           Jenkins, Hester Donaldson | Ibrahim Pasha  <blank>   51299  ...   \n",
       "\n",
       "     contents  instances                   genre audience authgender  \\\n",
       "213       NaN        1.0  fic | folklore | short      juv          m   \n",
       "43        NaN        1.0                     bio      NaN          m   \n",
       "454       NaN        2.0           fic | romance      NaN          f   \n",
       "455       NaN        2.0                     fic      NaN          m   \n",
       "83        NaN        1.0                     bio      NaN          m   \n",
       "66        NaN        NaN                     bio      NaN          m   \n",
       "129       NaN        1.0        fic | historical      NaN          m   \n",
       "21        NaN        1.0                     bio      NaN          f   \n",
       "369       NaN        2.0                     fic      NaN          m   \n",
       "77        NaN        NaN                     bio      NaN          f   \n",
       "\n",
       "     multiplehtids comments    coder               Folder  Trimmed  \n",
       "213            NaN      NaN      ted  gutenfictiontrimmed  Trimmed  \n",
       "43             NaN      NaN      ted      gutenbiotrimmed  Trimmed  \n",
       "454            NaN      NaN    wenyi  gutenfictiontrimmed  Trimmed  \n",
       "455            NaN      NaN  peizhen  gutenfictiontrimmed  Trimmed  \n",
       "83             NaN      NaN  peizhen      gutenbiotrimmed  Trimmed  \n",
       "66             NaN      NaN   morgan      gutenbiotrimmed  Trimmed  \n",
       "129            NaN      NaN  peizhen  gutenfictiontrimmed  Trimmed  \n",
       "21             NaN      NaN    wenyi      gutenbiotrimmed  Trimmed  \n",
       "369            NaN      NaN      ted  gutenfictiontrimmed  Trimmed  \n",
       "77             NaN      NaN   morgan      gutenbiotrimmed  Trimmed  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-twins",
   "metadata": {},
   "source": [
    "#### size of the smaller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "positive-ground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ourtitles.authgender == 'f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-steering",
   "metadata": {},
   "source": [
    "### Chunk metadata\n",
    "\n",
    "I've also produced chunk-level metadata, including a word error rate for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "saving-beauty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkmeta = pd.read_csv('../metadata/chunks_w_error_rates.tsv', sep = '\\t')\n",
    "chunkmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "reverse-ivory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gutenlen</th>\n",
       "      <th>hathilen</th>\n",
       "      <th>rejectedchars</th>\n",
       "      <th>wer</th>\n",
       "      <th>gbindex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>42147_0</td>\n",
       "      <td>89274</td>\n",
       "      <td>83894</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>42147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>54218_5</td>\n",
       "      <td>78340</td>\n",
       "      <td>76892</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030434</td>\n",
       "      <td>54218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>28271_1</td>\n",
       "      <td>72778</td>\n",
       "      <td>70995</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012405</td>\n",
       "      <td>28271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>37059_1</td>\n",
       "      <td>82548</td>\n",
       "      <td>89177</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>37059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>31210_2</td>\n",
       "      <td>76392</td>\n",
       "      <td>76864</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050161</td>\n",
       "      <td>31210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>52072_10</td>\n",
       "      <td>79877</td>\n",
       "      <td>77545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043375</td>\n",
       "      <td>52072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>41324_1</td>\n",
       "      <td>93766</td>\n",
       "      <td>97781</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>41324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>28366_3</td>\n",
       "      <td>81063</td>\n",
       "      <td>87463</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>28366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>51468_0</td>\n",
       "      <td>70936</td>\n",
       "      <td>74117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>51468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>41286_11</td>\n",
       "      <td>2469</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>41286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chunkid  gutenlen  hathilen  rejectedchars       wer gbindex\n",
       "954    42147_0     89274     83894              0  0.033099   42147\n",
       "1094   54218_5     78340     76892              0  0.030434   54218\n",
       "617    28271_1     72778     70995              0  0.012405   28271\n",
       "853    37059_1     82548     89177              0  0.083024   37059\n",
       "100    31210_2     76392     76864              0  0.050161   31210\n",
       "1083  52072_10     79877     77545              0  0.043375   52072\n",
       "948    41324_1     93766     97781              0  0.025723   41324\n",
       "482    28366_3     81063     87463              0  0.020563   28366\n",
       "442    51468_0     70936     74117              0  0.035709   51468\n",
       "163   41286_11      2469       179              0  0.993197   41286"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkmeta.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-address",
   "metadata": {},
   "source": [
    "There are still some very interesting things happening in the pipeline. *Mostly* we have very low word error rates, but there are a few extremely weird cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "utility-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkmeta = chunkmeta.drop_duplicates(subset = 'chunkid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "laden-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkmeta['gbindex'] = chunkmeta.chunkid.apply(get_gbindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "guilty-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmeta = chunkmeta.merge(ourtitles, how = 'inner', on = 'gbindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "environmental-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1237, 29)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "periodic-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chunkid', 'gutenlen', 'hathilen', 'rejectedchars', 'wer', 'gbindex',\n",
       "       'docid', 'author', 'authordate', 'title', 'latestcomp', 'hathidate',\n",
       "       'imprint', 'gutenstring', 'enumcron', 'nonficprob', 'juvenileprob',\n",
       "       'LOCgenres', 'LOCsubjects', 'contents', 'instances', 'genre',\n",
       "       'audience', 'authgender', 'multiplehtids', 'comments', 'coder',\n",
       "       'Folder', 'Trimmed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmeta.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-brisbane",
   "metadata": {},
   "source": [
    "### Balancing the distribution of classes across time\n",
    "\n",
    "Language change is very easy to model, so if you try to model the boundary between two categories that happen to be distributed differently across time (in your collection), you're very likely to get a model of language change. That's a problem if you want to study the categorical difference in itself, separated from confounding issues of chronology that might just be selection bias.\n",
    "\n",
    "Here we're distinguishing books written by men from those written by women. And although we don't really care about the model in its own right (since we're interested in the consequences of OCR distortion), it's still important to know what we're modeling, because OCR distortion could have *different* effects on different kinds of boundaries (e.g. chronological or demographic). \n",
    "\n",
    "So we need to be careful to balance the classes across time. At a minimum, we should require the median date for both categories to be roughly the same. A more ambitious approach would match the full distribution. But for right now let's keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "graphic-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_medians(smaller_indexes, larger_indexes, metadata):\n",
    "    '''\n",
    "    smaller_indexes = indexes of metadata for the smaller class\n",
    "    larger_indexes = indexes of metadata for the larger class\n",
    "    '''\n",
    "    selected_from_large = []\n",
    "    smaller_median = np.median(metadata.loc[smaller_indexes, 'latestcomp'])\n",
    "    \n",
    "    largerdf = metadata.loc[larger_indexes, : ]\n",
    "    above_median = largerdf.loc[largerdf['latestcomp'] >= smaller_median, : ].index.tolist()\n",
    "    below_median = largerdf.loc[largerdf['latestcomp'] <= smaller_median, : ].index.tolist()\n",
    "    \n",
    "    for i in range(len(smaller_indexes)):\n",
    "        if len(selected_from_large) > 0:\n",
    "            larger_median = np.median(metadata.loc[selected_from_large, 'latestcomp'])\n",
    "        else:\n",
    "            larger_median = smaller_median\n",
    "        \n",
    "        if larger_median >= smaller_median and len(below_median) > 0:\n",
    "            selected = random.sample(below_median, 1)[0]\n",
    "            below_median.pop(below_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        elif larger_median <= smaller_median and len(above_median) > 0:\n",
    "            selected = random.sample(above_median, 1)[0]\n",
    "            above_median.pop(above_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        else:\n",
    "            # we have no more items that won't distort the median\n",
    "            break\n",
    "\n",
    "    return selected_from_large, smaller_median, larger_median\n",
    "\n",
    "indexes_f = ourtitles.loc[ourtitles.authgender == 'f', : ].index.tolist()\n",
    "indexes_m = ourtitles.loc[ourtitles.authgender == 'm', : ].index.tolist()\n",
    "\n",
    "selected_m, median_f, median_m = match_medians(indexes_f, indexes_m, ourtitles)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abroad-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902.0 1900.5 81\n"
     ]
    }
   ],
   "source": [
    "print(median_f, median_m, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "lucky-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_selected_vols = indexes_f + selected\n",
    "len(all_selected_vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "measured-honduras",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2     27759\n",
       "8     32511\n",
       "10    32835\n",
       "12    33345\n",
       "14    33537\n",
       "20    35418\n",
       "21    35576\n",
       "24    36754\n",
       "25    36847\n",
       "27    36965\n",
       "Name: gbindex, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_gbindexes = ourtitles.loc[all_selected_vols, 'gbindex']\n",
    "print('We have ', len(selected_gbindexes))\n",
    "selected_gbindexes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "attractive-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(897, 29)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelmeta = fullmeta.loc[fullmeta['gbindex'].isin(selected_gbindexes), : ]\n",
    "modelmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "relative-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gutenlen</th>\n",
       "      <th>hathilen</th>\n",
       "      <th>rejectedchars</th>\n",
       "      <th>wer</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>contents</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37908_0</td>\n",
       "      <td>78086</td>\n",
       "      <td>78835</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064934</td>\n",
       "      <td>37908</td>\n",
       "      <td>wu.89006385306</td>\n",
       "      <td>Opie, Amelia Alderson</td>\n",
       "      <td>1769-1853.</td>\n",
       "      <td>Adeline Mowbray</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37908_1</td>\n",
       "      <td>78052</td>\n",
       "      <td>79125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083327</td>\n",
       "      <td>37908</td>\n",
       "      <td>wu.89006385306</td>\n",
       "      <td>Opie, Amelia Alderson</td>\n",
       "      <td>1769-1853.</td>\n",
       "      <td>Adeline Mowbray</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37908_2</td>\n",
       "      <td>78084</td>\n",
       "      <td>78744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051356</td>\n",
       "      <td>37908</td>\n",
       "      <td>wu.89006385306</td>\n",
       "      <td>Opie, Amelia Alderson</td>\n",
       "      <td>1769-1853.</td>\n",
       "      <td>Adeline Mowbray</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37908_3</td>\n",
       "      <td>78069</td>\n",
       "      <td>76923</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058616</td>\n",
       "      <td>37908</td>\n",
       "      <td>wu.89006385306</td>\n",
       "      <td>Opie, Amelia Alderson</td>\n",
       "      <td>1769-1853.</td>\n",
       "      <td>Adeline Mowbray</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37908_4</td>\n",
       "      <td>78042</td>\n",
       "      <td>79252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051276</td>\n",
       "      <td>37908</td>\n",
       "      <td>wu.89006385306</td>\n",
       "      <td>Opie, Amelia Alderson</td>\n",
       "      <td>1769-1853.</td>\n",
       "      <td>Adeline Mowbray</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wenyi</td>\n",
       "      <td>gutenfictiontrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunkid  gutenlen  hathilen  rejectedchars       wer gbindex  \\\n",
       "0  37908_0     78086     78835              0  0.064934   37908   \n",
       "1  37908_1     78052     79125              0  0.083327   37908   \n",
       "2  37908_2     78084     78744              0  0.051356   37908   \n",
       "3  37908_3     78069     76923              0  0.058616   37908   \n",
       "4  37908_4     78042     79252              0  0.051276   37908   \n",
       "\n",
       "            docid                 author  authordate            title  ...  \\\n",
       "0  wu.89006385306  Opie, Amelia Alderson  1769-1853.  Adeline Mowbray  ...   \n",
       "1  wu.89006385306  Opie, Amelia Alderson  1769-1853.  Adeline Mowbray  ...   \n",
       "2  wu.89006385306  Opie, Amelia Alderson  1769-1853.  Adeline Mowbray  ...   \n",
       "3  wu.89006385306  Opie, Amelia Alderson  1769-1853.  Adeline Mowbray  ...   \n",
       "4  wu.89006385306  Opie, Amelia Alderson  1769-1853.  Adeline Mowbray  ...   \n",
       "\n",
       "   contents  instances genre audience authgender  multiplehtids  comments  \\\n",
       "0       NaN        1.0   fic      NaN          f            NaN       NaN   \n",
       "1       NaN        1.0   fic      NaN          f            NaN       NaN   \n",
       "2       NaN        1.0   fic      NaN          f            NaN       NaN   \n",
       "3       NaN        1.0   fic      NaN          f            NaN       NaN   \n",
       "4       NaN        1.0   fic      NaN          f            NaN       NaN   \n",
       "\n",
       "   coder               Folder  Trimmed  \n",
       "0  wenyi  gutenfictiontrimmed  Trimmed  \n",
       "1  wenyi  gutenfictiontrimmed  Trimmed  \n",
       "2  wenyi  gutenfictiontrimmed  Trimmed  \n",
       "3  wenyi  gutenfictiontrimmed  Trimmed  \n",
       "4  wenyi  gutenfictiontrimmed  Trimmed  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-phoenix",
   "metadata": {},
   "source": [
    "### Term-document matrices for clean and dirty narratives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "considered-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rootdir = '/Users/tunder/Box Sync/NEHproject/cleannarratives/'\n",
    "dirty_rootdir = '/Users/tunder/Box Sync/NEHproject/dirtynarratives/'\n",
    "\n",
    "clean_paths = []\n",
    "dirty_paths = []\n",
    "\n",
    "for chunk_id in modelmeta['chunkid']:\n",
    "    clean_paths.append(clean_rootdir + chunk_id + '.txt')\n",
    "    dirty_paths.append(dirty_rootdir + chunk_id + '.txt')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "junior-universe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>12th</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourselves</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zest</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37908_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000  10  100  10th  11  11th  12  12th  13  13th  ...  yours  \\\n",
       "37908_0    0   0    0     0   0     0   0     0   0     0  ...      1   \n",
       "37908_1    0   0    0     0   0     0   0     0   0     0  ...      3   \n",
       "37908_2    0   0    0     0   0     0   0     0   0     0  ...      6   \n",
       "37908_3    0   0    0     0   0     0   0     0   0     0  ...      3   \n",
       "37908_4    0   0    0     0   0     0   0     0   0     0  ...      2   \n",
       "\n",
       "         yourself  yourselves  youth  youthful  youths  zeal  zealous  zest  \\\n",
       "37908_0         1           0      1         2       0     0        0     0   \n",
       "37908_1         5           0      2         0       0     0        0     0   \n",
       "37908_2         2           1      2         0       0     0        0     0   \n",
       "37908_3         5           0      1         0       0     0        0     0   \n",
       "37908_4        11           0      2         0       0     0        0     0   \n",
       "\n",
       "         zone  \n",
       "37908_0     0  \n",
       "37908_1     0  \n",
       "37908_2     0  \n",
       "37908_3     0  \n",
       "37908_4     0  \n",
       "\n",
       "[5 rows x 11846 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'chunkid' in modelmeta.columns:         \n",
    "    model_chunk_ids = modelmeta['chunkid']\n",
    "    modelmeta = modelmeta.set_index('chunkid')   # If we haven't made this the index yet, let's do it.\n",
    "else:\n",
    "    model_chunk_ids = modelmeta.index.tolist()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(input = 'filename', min_df = .04)\n",
    "sparse_clean_counts = vectorizer.fit_transform(clean_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "clean_wordcounts = pd.DataFrame(sparse_clean_counts.toarray(), index = model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "clean_wordcounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "absolute-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourselves</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zest</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37908_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000  10  100  101  102  103  104  105  106  107  ...  yours  \\\n",
       "37908_0    0   0    0    0    0    0    0    0    0    0  ...      1   \n",
       "37908_1    0   0    1    0    0    0    1    1    1    0  ...      3   \n",
       "37908_2    0   0    0    0    0    0    0    0    0    0  ...      6   \n",
       "37908_3    0   0    0    1    1    0    0    1    1    1  ...      3   \n",
       "37908_4    0   0    0    0    0    0    0    0    0    0  ...      2   \n",
       "\n",
       "         yourself  yourselves  youth  youthful  youths  zeal  zealous  zest  \\\n",
       "37908_0         1           0      1         2       0     0        0     0   \n",
       "37908_1         3           0      2         0       0     0        0     0   \n",
       "37908_2         2           1      2         0       0     0        0     0   \n",
       "37908_3         5           0      1         0       0     0        0     0   \n",
       "37908_4        10           0      2         0       0     0        0     0   \n",
       "\n",
       "         zone  \n",
       "37908_0     0  \n",
       "37908_1     0  \n",
       "37908_2     0  \n",
       "37908_3     0  \n",
       "37908_4     0  \n",
       "\n",
       "[5 rows x 12556 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input = 'filename', min_df = .04)\n",
    "sparse_dirty_counts = vectorizer.fit_transform(dirty_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "dirty_wordcounts = pd.DataFrame(sparse_dirty_counts.toarray(), index = model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "dirty_wordcounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "analyzed-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rowsums = clean_wordcounts.sum(axis = 'columns')\n",
    "clean_freqs = clean_wordcounts.divide(clean_rowsums, axis = 'rows')\n",
    "scaler = StandardScaler()\n",
    "clean_freqs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "interstate-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_rowsums = dirty_wordcounts.sum(axis = 'columns')\n",
    "dirty_freqs = dirty_wordcounts.divide(clean_rowsums, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "numeric-mayor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourselves</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zest</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37908_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000   10       100       101       102  103       104       105  \\\n",
       "37908_0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "37908_1  0.0  0.0  0.000081  0.000000  0.000000  0.0  0.000081  0.000081   \n",
       "37908_2  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "37908_3  0.0  0.0  0.000000  0.000081  0.000081  0.0  0.000000  0.000081   \n",
       "37908_4  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "              106       107  ...     yours  yourself  yourselves     youth  \\\n",
       "37908_0  0.000000  0.000000  ...  0.000081  0.000081    0.000000  0.000081   \n",
       "37908_1  0.000081  0.000000  ...  0.000244  0.000244    0.000000  0.000163   \n",
       "37908_2  0.000000  0.000000  ...  0.000489  0.000163    0.000081  0.000163   \n",
       "37908_3  0.000081  0.000081  ...  0.000242  0.000403    0.000000  0.000081   \n",
       "37908_4  0.000000  0.000000  ...  0.000163  0.000817    0.000000  0.000163   \n",
       "\n",
       "         youthful  youths  zeal  zealous  zest  zone  \n",
       "37908_0  0.000163     0.0   0.0      0.0   0.0   0.0  \n",
       "37908_1  0.000000     0.0   0.0      0.0   0.0   0.0  \n",
       "37908_2  0.000000     0.0   0.0      0.0   0.0   0.0  \n",
       "37908_3  0.000000     0.0   0.0      0.0   0.0   0.0  \n",
       "37908_4  0.000000     0.0   0.0      0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 12556 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_freqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-niger",
   "metadata": {},
   "source": [
    "### Let's produce a model for the clean counts\n",
    "\n",
    "We're going to do a grid search for the best model. The outer loop will select the number of features. The inner loop will select the regularization constant.\n",
    "\n",
    "In selecting the top *n* features, we will always select the *n* with top *document* frequency.\n",
    "\n",
    "Eventually we're going to want to separate the train-and-validation loop from a real, separate test set. But we don't have all the data yet; still processing; so it doesn't make a lot of sense to discuss that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "monthly-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1 0.6165792759051185\n",
      "2000 0.1 0.6232833957553059\n",
      "2000 0.01 0.6333333333333333\n",
      "2000 0.001 0.6500873907615481\n",
      "2000 0.0001 0.6712983770287141\n",
      "2000 1e-05 0.5116853932584269\n",
      "2000 1e-06 0.5094631710362046\n",
      "3000 1 0.6456803995006243\n",
      "3000 0.1 0.6445692883895131\n",
      "3000 0.01 0.6479151061173534\n",
      "3000 0.001 0.6546192259675405\n",
      "3000 0.0001 0.672434456928839\n",
      "3000 1e-05 0.5428464419475656\n",
      "3000 1e-06 0.5094631710362046\n",
      "4000 1 0.6779525593008738\n",
      "4000 0.1 0.6734956304619226\n",
      "4000 0.01 0.6757303370786517\n",
      "4000 0.001 0.6857677902621724\n",
      "4000 0.0001 0.6980524344569288\n",
      "4000 1e-05 0.5952434456928838\n",
      "4000 1e-06 0.5094631710362046\n",
      "5000 1 0.7024594257178527\n",
      "5000 0.1 0.7024594257178527\n",
      "5000 0.01 0.7013233458177279\n",
      "5000 0.001 0.7203245942571785\n",
      "5000 0.0001 0.7326217228464419\n",
      "5000 1e-05 0.6287141073657928\n",
      "5000 1e-06 0.5094631710362046\n",
      "6000 1 0.7213732833957552\n",
      "6000 0.1 0.7224843945068663\n",
      "6000 0.01 0.7258426966292134\n",
      "6000 0.001 0.723645443196005\n",
      "6000 0.0001 0.732621722846442\n",
      "6000 1e-05 0.676629213483146\n",
      "6000 1e-06 0.5094631710362046\n",
      "7000 1 0.7369662921348314\n",
      "7000 0.1 0.7358551810237204\n",
      "7000 0.01 0.7347315855181024\n",
      "7000 0.001 0.7370037453183521\n",
      "7000 0.0001 0.7225717852684144\n",
      "7000 1e-05 0.705605493133583\n",
      "7000 1e-06 0.5094631710362046\n"
     ]
    }
   ],
   "source": [
    "for featurecount in [2000, 3000, 4000, 5000, 6000, 7000]:\n",
    "    docfreqs = []\n",
    "    for col in clean_freqs.columns:\n",
    "        docfreqs.append((sum(clean_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[0: featurecount]]\n",
    "    \n",
    "    model_features = clean_freqs.loc[ : , features]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model_features = scaler.fit_transform(model_features)\n",
    "    \n",
    "    for c_param in [1, .1, .01, .001, .0001, .00001, .000001]:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 1000)\n",
    "        \n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(logreg, model_features, modelmeta['authgender'], groups = modelmeta['author'], cv = grouper)\n",
    "        cv_results = cross_validate(logreg, model_features, modelmeta['authgender'], cv = 10)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "neural-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 1 0.7024843945068664\n",
      "6000 0.1 0.7192134831460675\n",
      "6000 0.01 0.7258551810237204\n",
      "6000 0.001 0.7258676654182272\n",
      "6000 0.0001 0.720287141073658\n",
      "6000 1e-05 0.6621847690387016\n",
      "6000 1e-06 0.5094631710362046\n",
      "7000 1 0.7035705368289638\n",
      "7000 0.1 0.7180524344569287\n",
      "7000 0.01 0.7280774032459426\n",
      "7000 0.001 0.7380898876404495\n",
      "7000 0.0001 0.7280898876404494\n",
      "7000 1e-05 0.6844694132334581\n",
      "7000 1e-06 0.5094631710362046\n"
     ]
    }
   ],
   "source": [
    "for featurecount in [6000, 7000]:\n",
    "    docfreqs = []\n",
    "    for col in dirty_freqs.columns:\n",
    "        docfreqs.append((sum(dirty_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[0: featurecount]]\n",
    "    \n",
    "    model_features = dirty_freqs.loc[ : , features]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model_features = scaler.fit_transform(model_features)\n",
    "    \n",
    "    for c_param in [1, .1, .01, .001, .0001, .00001, .000001]:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 1000)\n",
    "        \n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(logreg, model_features, modelmeta['authgender'], groups = modelmeta['author'], cv = grouper)\n",
    "        cv_results = cross_validate(logreg, model_features, modelmeta['authgender'], cv = 10)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "conservative-lightning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid           author authordate              title  \\\n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "\n",
       "   latestcomp  hathidate                       imprint  \\\n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "\n",
       "                           gutenstring enumcron gbindex  ...  instances  \\\n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "\n",
       "   genre audience authgender multiplehtids  comments   coder           Folder  \\\n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "\n",
       "   Trimmed     filename  \n",
       "0  Trimmed  27553_5.txt  \n",
       "0  Trimmed  27553_4.txt  \n",
       "0  Trimmed  27553_6.txt  \n",
       "0  Trimmed  27553_7.txt  \n",
       "0  Trimmed  27553_3.txt  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkmeta = []\n",
    "\n",
    "for idx, row in ourtitles.iterrows():\n",
    "    \n",
    "    gbindex = row['gbindex']\n",
    "    files4thisindex = gbdict[gbindex]\n",
    "    \n",
    "    for filename in files4thisindex:\n",
    "        chunkrow = pd.Series(row)\n",
    "        chunkrow['filename'] = filename\n",
    "        chunkmeta.append(chunkrow)\n",
    "\n",
    "chunkmeta = pd.DataFrame(chunkmeta)\n",
    "chunkmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-antigua",
   "metadata": {},
   "source": [
    "### Analysis plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-daughter",
   "metadata": {},
   "source": [
    "We need to optimize the number of features and the regularization constant. But we don't want to overfit a particular sample. (There is in reality not a huge danger of overfitting with two parameters, but since we're trying to determine the risk of distortion as precisely as possible, it's best to be scrupulous here.)\n",
    "\n",
    "So I would propose that we separate our test set (1/4 of the data) from 3/4 of the data that we use for training-and-validation. That's to say, we cross-validate and optimize on 3/4 of the data (the same 3/4 for both clean and dirty corpora). And then finally test the model produced by that 3/4 on a held-out 1/4 of the data.\n",
    "\n",
    "The one additional complication is that we need to be dividing the data *by author.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abandoned-labor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(chunkmeta['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "elementary-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbott, Henry\n",
      "Abbott, Jacob,\n",
      "Alcott, Louisa May,\n",
      "Allen, George Hoyt\n",
      "Allen, Grant,\n",
      "Barine, Arvède,\n",
      "Barrie, J. M.\n",
      "Bates, Arlo\n",
      "Beerbohm, Julius\n",
      "Bell, Henry Glassford,\n",
      "Bettany, George Thomas,\n",
      "Blind, Mathilde,\n",
      "Blunden, Edmund\n",
      "Broun, Heywood\n",
      "Brown, E. E.\n",
      "Burney, Fanny\n",
      "Campan, Mme\n",
      "Carter, Harry\n",
      "Casson, Herbert Newton,\n",
      "Castlemon, Harry\n",
      "Cibber, Colley\n",
      "Colmache, Édouard\n",
      "Coolidge, Susan\n",
      "Cooper, James Fenimore\n",
      "Crockett, David\n",
      "Cruttwell, Maud.\n",
      "Daniels, Mabel W.\n",
      "Duffy, Bella\n",
      "Duncan, Norman\n",
      "Farrar, Geraldine.\n",
      "Gallishaw, John\n",
      "Gaskell, Elizabeth Cleghorn\n",
      "Gay, Sydney Howard,\n",
      "Giberne, Agnes\n",
      "Gilchrist, Anne Burrows,\n",
      "Goodwin, William\n",
      "Graham, Isabella\n",
      "Grimaldi, Joseph,\n",
      "Gronniosaw, James Albert\n",
      "Habberton, John\n",
      "Haldane, Elizabeth Sanderson,\n",
      "Hall, Edward B.\n",
      "Harland, Marion,\n",
      "Harrison, James\n",
      "Haynes, Henrietta.\n",
      "Higginson, Thomas Wentworth,\n",
      "Horne, C. Silvester\n",
      "Howe, Julia Ward,\n",
      "Hughes, Thomas\n",
      "Ingemann, Bernhard Severin\n",
      "Ingoldsby, Thomas\n",
      "Jenkins, Hester Donaldson,\n",
      "Johnson, Willis Fletcher,\n",
      "Keller, Elizabeth Leavitt\n",
      "Kemble, Fanny,\n",
      "Kennard, Nina H.,\n",
      "Kennedy, John Pendleton\n",
      "Kippis, Andrew\n",
      "Larsen, Hanna Astrup,\n",
      "Lever, Charles James\n",
      "Levinger, Lee J.\n",
      "Levy, T. Aaron,\n",
      "Longstreet, James,\n",
      "Loria, Achille,\n",
      "MacLane, Mary\n",
      "Marrs, William Taylor\n",
      "Marshall, John\n",
      "Mayer, Brantz,\n",
      "Miller, Florence Fenwick,\n",
      "Mixson, Frank M\n",
      "Moore, Thomas\n",
      "Morlae, Edward\n",
      "Morris, Clara,\n",
      "Moscheles, Felix,\n",
      "Moses, Belle.\n",
      "Norgate, Kate.\n",
      "O'Neil, Owen Rowe\n",
      "Oliphant, Mrs. (Margaret)\n",
      "Opie, Amelia Alderson\n",
      "Perryman, F. M\n",
      "Pompadour, Jeanne Antoinette\n",
      "Reed, John\n",
      "Reid, Mayne\n",
      "Reid, Stuart J.\n",
      "Reynolds, Robert Rice\n",
      "Roosevelt, Theodore,\n",
      "Ruxton, George Frederick Augustus\n",
      "Saxton, Evelyn\n",
      "Scoville, Samuel\n",
      "Sessions, Frederick.\n",
      "Shelley, Mary Wollstonecraft\n",
      "Sheridan, Clare\n",
      "Smeaton, William Henry Oliphant\n",
      "Smiles, Samuel,\n",
      "Smith, Lucy,\n",
      "Sterne, Elaine\n",
      "Stoddard, Charles Warren\n",
      "Stone, William L.\n",
      "Stowe, Harriet Beecher\n",
      "Stowe, Harriet Beecher,\n",
      "Stratton, Royal B\n",
      "Stretton, Hesba\n",
      "Tappan, Eva March\n",
      "Taylor, Meadows\n",
      "Tetley, William C\n",
      "Thoreau, Henry David\n",
      "Thwaites, Reuben Gold,\n",
      "Torrey, Bradford\n",
      "Trollope, Anthony\n",
      "Tyndall, John,\n",
      "Whiting, Lilian,\n",
      "Wordsworth, Dorothy,\n",
      "Yeats-Brown, Francis,\n",
      "de Stael, Germaine\n"
     ]
    }
   ],
   "source": [
    "authors = list(set(chunkmeta['author']))\n",
    "authors.sort()\n",
    "for a in authors:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-relative",
   "metadata": {},
   "source": [
    "Our next problem to resolve is, check out Stowe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-mounting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
